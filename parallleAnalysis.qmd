---
title: "Parallel Analysis theory"
format: html
editor: visual
bibliography: references.bib
---

## PA theory

In parallel analysis we are interested in finding the true amount of factors. In practice, this is arguably not possible with observed data without assumptions. In psychometric theory, the assumptions often is that responses to a set of items are due to traits or other latent aspects of the respondent. The other assumption is that there is also error within each item. Most of the time this error is assumed independent.

Independent variables, i.d. with mean 0 and observed variance of 1 are used here. This means that they have a correlation matrix $\Sigma$ with identity diagonal and off-diagonal elements of $-1\le r_{i,j}\le 1$. The complication arises that while the observed variance is 1 after standardizing, we do not know how much error variance (assumed to exist) is included in each variable. We'll use a hypothetical scenario. Say you have 0.5 error variance in each variable, and they are independent. This would mean that there exists as many factors as there are variables, each loaded $\sqrt{.5}\sigma^{-1}$ on their respective variable where $\sigma$ is the respective factor variance. As a factor model, it would not be identified, but it still would be the true model. Also the parallel analysis is a solution to sampling error induced in estimating the number of factors, but we'll deal with the asymptote $N\to\infty$ so the Kaiser's criteria, for example,

Say a factor has 4 indicators (items) - variables loaded on the factor. Say each variable has 0.5 error variance again, and hence the loadings are approximately 0.7071, assuming factor variance of 1. Variables are correlated with correlation 0.7071\^2 = 0.5 approximately. Now, removing the error variance from the correlation matrix diagonal, the correlation matrix becomes:

$$
\begin{pmatrix}
0.5&0.5&0.5&0.5\\
0.5&0.5&0.5&0.5\\
0.5&0.5&0.5&0.5\\
0.5&0.5&0.5&0.5\\
\end{pmatrix}
$$

This matrix has eigenvalues of 0,0,0,2. Lets move to a 2 factor scenario. Assume the correlation matrix

$$
\begin{pmatrix}
0.5&0.5&0.5&0.5&0&0&0&0\\
0.5&0.5&0.5&0.5&0&0&0&0\\
0.5&0.5&0.5&0.5&0&0&0&0\\
0.5&0.5&0.5&0.5&0&0&0&0\\
0&0&0&0&0.5&0.5&0.5&0.5\\
0&0&0&0&0.5&0.5&0.5&0.5\\
0&0&0&0&0.5&0.5&0.5&0.5\\
0&0&0&0&0.5&0.5&0.5&0.5\\
\end{pmatrix}
$$

which has eigenvalues of 0,0,0,0,0,0,2,2. Now assume that you have two correlated factors with correaltion of 0.5. This makes correlations across factor items - items x,y loaded to different factors

$$
\text{Cov}(X,Y)=E[XY]=E[(0.701\eta_X+\psi_X)(0.701\eta_Y+\psi_Y)]=\\
E[0.701^2\eta_X\eta_Y+0.701\eta_X\psi_Y+0.701\eta_Y\psi_X+\psi_X\psi_Y ]=\\
0.701^2\times0.5\approx 0.25=:0.25 \text{ for this example}
$$

$$
\begin{pmatrix}
0.5&0.5&0.5&0.5&0.25&0.25&0.25&0.25\\
0.5&0.5&0.5&0.5&0.25&0.25&0.25&0.25\\
0.5&0.5&0.5&0.5&0.25&0.25&0.25&0.25\\
0.5&0.5&0.5&0.5&0.25&0.25&0.25&0.25\\
0.25&0.25&0.25&0.25&0.5&0.5&0.5&0.5\\
0.25&0.25&0.25&0.25&0.5&0.5&0.5&0.5\\
0.25&0.25&0.25&0.25&0.5&0.5&0.5&0.5\\
0.25&0.25&0.25&0.25&0.5&0.5&0.5&0.5\\
\end{pmatrix}
$$

this matrix has eigenvalues of 0, 0, 0, 0, 0, 0, 1, 3 and 1.5, 3.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5 if we didnt adjust the diagonal - i.e., use the unadjusted method with identity diagonal. We see that there are positive eigenvalues as many as there are factors and otherwise zero eigenvalues for the adjusted method. For the non-adjusted method we see that the error variances exist in all of the eigenvalues (we know that there are 0.5 error variance for every variable). Using each the unadjusted method with criteria eigenvalue \> 0, or the original (Kaiser) criteria for the unadjusted method of eigenvalue \> 1 works.

The methods start to practically deviate and give different suggestions for the number of factors when correlations between factors get strong. Assume a stronger correlation of 0.7 between the two factors. The correlation between variables across sets X and Y now is $0.5\times0.7=0.35$. The correlation matrix is

$$
\begin{pmatrix}
0.5&0.5&0.5&0.5&0.35&0.35&0.35&0.35\\
0.5&0.5&0.5&0.5&0.35&0.35&0.35&0.35\\
0.5&0.5&0.5&0.5&0.35&0.35&0.35&0.35\\
0.5&0.5&0.5&0.5&0.35&0.35&0.35&0.35\\
0.35&0.35&0.35&0.35&0.5&0.5&0.5&0.5\\
0.35&0.35&0.35&0.35&0.5&0.5&0.5&0.5\\
0.35&0.35&0.35&0.35&0.5&0.5&0.5&0.5\\
0.35&0.35&0.35&0.35&0.5&0.5&0.5&0.5\\
\end{pmatrix}
$$

with eigenvalues 0, 0, 0, 0, 0, 0.6, 3.4 $\to$ retain 2 factors. Using identity diagonal the eigenvalues are 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.1, 3.9 $\to$ retain 2 factors. Notice that the second eigenvalue above criteria starts to close in on the cut-off. Assuming correlation of 0.8 with the same calculations we'd get the eigenvalues of 0, 0, 0, 0, 0, 0, 0.4, 3.6, 2 are positive $\to$ retain 2 factors, and with identity diagonal 0.9, 4.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5 one is above 1 $\to$ retain 1 factor. Assuming correlation of 0.9 we'd get the eigenvalues of 0, 0, 0, 0, 0, 0, 0.2, 3.8 and with identity diagonal 0.7, 4.3, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5. It seems then that the non-adjusted method begins to fail when there is a strong correlation between the factors in a two factor scenario. We'll skip further complicating the analysis into more factors. (Also I'm not sure if assumption of unit variances makes a difference.)

Communality adjusted method would not fail using retention criteria of \> 0, non-adjusted method using \> 1 criteria would fail. Why they differ? The central observation might be that (of course) the eigenvalues are representative of each error variance and factor variances in the non-adjusted method. The differences in the adjusted matrix eigenvalues $\lambda_{adj}$ and non-adjusted matrix eigenvalues $\lambda$ seems to be

$$
\lambda_{adj}-\lambda=\psi
$$

The adjusted criteria for the unadjusted matrix should then be

$$
\lambda>1-\psi
$$

This criteria would've performed optimally in the above scenarios, even with strongly correlated factors. (In the above a constant error variance for all items is assumed. It is not precise.) Also the above results hinted that unadjusted method can only underfactor - not overfactor. This is because the criteria needs to be shifted down, never up (it is maximal at $\psi=0$).

More precisely (maybe) if there are orthogonal unique directions within the correlation matrix ('errors') as well as factors, that can be oblique w.r.t. other factors, but orthogonal to the errors, then each unique directions must have an eigenvalue of $\lambda_u\ge \text{Var}(\psi)$. This is because there are at most $K$ orthogonal directions in the correlation matrix. This means that if we subtract the error variances of all $\lambda$ we obtain eigenvalues uninflated by the unique directions. This means that the adjusted eigenvalues $\lambda_{i,adj}=\lambda_i-\text{Var}(\psi_i)$ must only give the magnitude the factor directions. Without the adjustment, we'd have eigenvalues that have values which depend on an unknown amount of error variance and an unknown amount of factor variance.

This means that, if we know error variance, we can then adjust accordingly (using either method). We do not know error variance, and are left to estimate it. Estimating error variance can be done in multiple ways. E.g. by fitting a 1 factor model. After estimating it you can use the adjusted matrix with criteria \> 0 or the non-adjusted matrix, but adjusted criteria ($>1-\psi$ ). If the criteria is not adjusted for the non-adjusted matrix then the larger the smaller the error variance, the more likely we underfactor. These are of course theoretical values, not including sampling errors central to the tenet of parallel analysis. Random sampling methods have been implemented for the adjusted method.

[@steger2006][@crawford2010] discuss factor retention criteria and favor the communality adjusted method when there are correlated factors. Revelle discusses that 1-fa based communality adjustment might be reasonable, as it does not overfactor as much as squared mean correlation communality adjustment. All packages that I'm aware of (paran, psych, hornpa, parallel) use either of the above mentioned methods or similar.

### Final personal note

When deciding how many factors to retain, purpose seems most important. Communality adjustment can be done in many ways, none of which will be guaranteed to work - even decently perhaps. Horn's method is apparently OK in most cases (at least people make simulations which tell its OK, although some say that their own method is better [@doi.org/10.1371/journal.pone.0174035]) unless (multiple) strongly correlated factors are expected, as is perhaps in psychopathology research causing a problem maybe.
